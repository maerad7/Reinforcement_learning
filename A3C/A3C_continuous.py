# ===============================================================
# ğŸ§  A3C (Asynchronous Advantage Actor-Critic) with PyTorch
#      - Continuous Action (Pendulum-v1)
#      - Multi-threaded (CPU cores)
# ===============================================================

import os
import math
import time
import threading
from threading import Thread, Lock
from multiprocessing import cpu_count
from typing import Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gym
from gym.wrappers import RecordVideo
import pathlib
# ===============================================================
# ğŸ”§ ê¸°ë³¸ ì„¤ì •ê°’
# ===============================================================
actor_lr = 5e-4            # Actor í•™ìŠµë¥ 
critic_lr = 1e-3           # Critic í•™ìŠµë¥ 
gamma = 0.99               # í• ì¸ìœ¨ (discount factor)
hidden_size = 128          # ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜
update_interval = 50       # ëª‡ stepë§ˆë‹¤ ê¸€ë¡œë²Œ ì—…ë°ì´íŠ¸í• ì§€
max_episodes = 500         # ì „ì²´ í•™ìŠµ episode ìˆ˜
entropy_beta = 1e-3        # ì—”íŠ¸ë¡œí”¼ í•­ ê°€ì¤‘ì¹˜ (íƒí—˜ì„±)
grad_clip = 5.0            # gradient clipping í•œê³„
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # ì¥ì¹˜ ì„ íƒ
torch.set_default_dtype(torch.float64)  # float64 ì •ë°€ë„ ì‚¬ìš© (TensorFlow ì½”ë“œ í˜¸í™˜)

GLOBAL_EP = 0              # ì „ì—­ episode ì¹´ìš´í„°
GLOBAL_EP_LOCK = Lock()    # ìŠ¤ë ˆë“œ ë™ê¸°í™”ìš© Lock
PRINT_LOCK = Lock()        # ì¶œë ¥ ë™ê¸°í™”ìš© Lock (ì¶œë ¥ ê¼¬ì„ ë°©ì§€)

# ===============================================================
# ğŸ§© Gym API ë²„ì „ í˜¸í™˜ í—¬í¼
# ===============================================================
def reset_env(env):
    """Gym ë²„ì „ì— ë”°ë¼ reset() ë°˜í™˜ê°’ì´ (obs, info)ì¸ ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ í˜¸í™˜ ì²˜ë¦¬"""
    out = env.reset()
    if isinstance(out, tuple):
        s, info = out
        return s
    return out

def step_env(env, action) -> Tuple[np.ndarray, float, bool, dict]:
    """Gym ë²„ì „ë³„ step() ë°˜í™˜ê°’ í˜¸í™˜"""
    out = env.step(action)
    if len(out) == 5:  # Gym v26+: (obs, reward, terminated, truncated, info)
        ns, r, term, trunc, info = out
        done = bool(term or trunc)
        return ns, r, done, info
    else:              # Old API: (obs, reward, done, info)
        ns, r, done, info = out
        return ns, r, bool(done), info


# ===============================================================
# ğŸ§± Actor (ì •ì±… ë„¤íŠ¸ì›Œí¬)
# ===============================================================
class Actor(nn.Module):
    def __init__(self, state_size, action_size, action_bound):
        super().__init__()
        self.action_bound = float(action_bound)  # í™˜ê²½ì˜ ì•¡ì…˜ ìµœëŒ€ê°’

        # ë‘ ê°œì˜ ì€ë‹‰ì¸µ (ReLU í™œì„±í™”)
        self.net = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
        )
        # í‰ê· (Î¼)ê³¼ ë¡œê·¸í‘œì¤€í¸ì°¨(logÏƒ)ë¥¼ ì¶œë ¥í•˜ëŠ” ë‘ ê°œì˜ í—¤ë“œ
        self.mu_head = nn.Linear(hidden_size, action_size)
        self.log_std_head = nn.Linear(hidden_size, action_size)

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (He initialization)
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
                nn.init.zeros_(m.bias)

    def forward(self, x):
        """ì…ë ¥ ìƒíƒœ â†’ Î¼, Ïƒ ì¶œë ¥"""
        h = self.net(x)
        mu = torch.tanh(self.mu_head(h)) * self.action_bound  # tanh â†’ [-1,1] â†’ ë²”ìœ„ ìŠ¤ì¼€ì¼
        log_std = self.log_std_head(h)
        # log_std í´ë¨í•‘: ë„ˆë¬´ ì‘ì€ std(í­ë°œ ë°©ì§€)
        log_std = torch.clamp(log_std, math.log(1e-2), math.log(1.0))
        std = torch.exp(log_std)
        return mu, std

    def sample_action(self, state_tensor):
        """í˜„ì¬ ì •ì±…ì—ì„œ ì•¡ì…˜ ìƒ˜í”Œë§"""
        with torch.no_grad():
            mu, std = self(state_tensor)
            dist = torch.distributions.Normal(mu, std)
            a = dist.sample()  # ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œ
            logp = dist.log_prob(a).sum(dim=-1, keepdim=True)  # ë¡œê·¸ í™•ë¥  (ì†ì‹¤ ê³„ì‚°ìš©)
            # í™˜ê²½ì˜ ì•¡ì…˜ ë²”ìœ„ë¡œ í´ë¦¬í•‘
            a = torch.clamp(a, -self.action_bound, self.action_bound)
        return a.cpu().numpy()[0], logp

    def log_prob_and_entropy(self, states, actions):
        """ì •ì±… ë¡œê·¸í™•ë¥ ê³¼ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        mu, std = self(states)
        dist = torch.distributions.Normal(mu, std)
        logp = dist.log_prob(actions).sum(dim=-1, keepdim=True)   # log Ï€(a|s)
        ent = dist.entropy().sum(dim=-1, keepdim=True)            # ì—”íŠ¸ë¡œí”¼(íƒí—˜ì„±)
        return logp, ent


# ===============================================================
# ğŸ§± Critic (ê°€ì¹˜ í•¨ìˆ˜)
# ===============================================================
class Critic(nn.Module):
    def __init__(self, state_size):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1),
        )
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
                nn.init.zeros_(m.bias)

    def forward(self, x):
        """ìƒíƒœ ê°€ì¹˜ V(s) ë°˜í™˜"""
        return self.net(x)


# ===============================================================
# ğŸŒ Global A3C: ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ ë° ì˜µí‹°ë§ˆì´ì € ê´€ë¦¬
# ===============================================================
class GlobalA3C:
    def __init__(self, env_name: str):
        env = gym.make(env_name)
        self.env_name = env_name
        self.state_size = env.observation_space.shape[0]
        self.action_size = env.action_space.shape[0]
        self.action_bound = float(env.action_space.high[0])

        # ê¸€ë¡œë²Œ Actor & Critic ìƒì„±
        self.actor = Actor(self.state_size, self.action_size, self.action_bound).to(device)
        self.critic = Critic(self.state_size).to(device)

        # ì˜µí‹°ë§ˆì´ì € ê°ê° ë¶„ë¦¬ (TensorFlow ë²„ì „ê³¼ ë™ì¼)
        self.actor_opt = optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_opt = optim.Adam(self.critic.parameters(), lr=critic_lr)

        # ìŠ¤ë ˆë“œ ë™ê¸°í™”ë¥¼ ìœ„í•œ Lock
        self.update_lock = Lock()

    def apply_grads(self, actor_loss, critic_loss, actor_params, critic_params):
        """ì›Œì»¤ê°€ ê³„ì‚°í•œ lossë¥¼ ì´ìš©í•´ ê¸€ë¡œë²Œ íŒŒë¼ë¯¸í„° ê°±ì‹ """
        with self.update_lock:
            # ---- Actor ----
            self.actor_opt.zero_grad()
            actor_loss.backward(retain_graph=False)
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), grad_clip)
            self.actor_opt.step()

            # ---- Critic ----
            self.critic_opt.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), grad_clip)
            self.critic_opt.step()

    def sync_to_local(self, local_actor: Actor, local_critic: Critic):
        """ê¸€ë¡œë²Œ â†’ ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ë¡œ íŒŒë¼ë¯¸í„° ë³µì‚¬"""
        local_actor.load_state_dict(self.actor.state_dict())
        local_critic.load_state_dict(self.critic.state_dict())

        # ëª¨ë¸ ì €ì¥ìš© í•¨ìˆ˜ (ì„ íƒ)

    def save(self, actor_path: str, critic_path: str):
        os.makedirs("Video/continuous", exist_ok=True)

        torch.save(self.actor.state_dict(), os.path.join("Video/continuous", actor_path))
        torch.save(self.critic.state_dict(), os.path.join("Video/continuous", critic_path))

    def load(self, actor_path: str, critic_path: str, map_location=None):
        map_location = map_location or device
        self.actor.load_state_dict(torch.load(actor_path, map_location=map_location))
        self.critic.load_state_dict(torch.load(critic_path, map_location=map_location))
        self.actor.to(device).eval()
        self.critic.to(device).eval()

# ===============================================================
# ğŸ§µ Worker: ë³‘ë ¬ í•™ìŠµ ìŠ¤ë ˆë“œ
# ===============================================================
class Worker(Thread):
    def __init__(self, wid: int, env_name: str, global_agent: GlobalA3C):
        super().__init__(daemon=True)
        self.wid = wid
        self.name = f"w{wid}"
        self.env = gym.make(env_name)
        self.global_agent = global_agent

        self.state_size = self.env.observation_space.shape[0]
        self.action_size = self.env.action_space.shape[0]
        self.action_bound = float(self.env.action_space.high[0])

        # ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ (ì´ˆê¸°ì—” ê¸€ë¡œë²Œê³¼ ë™ì¼í•˜ê²Œ ì‹œì‘)
        self.actor = Actor(self.state_size, self.action_size, self.action_bound).to(device)
        self.critic = Critic(self.state_size).to(device)
        self.sync_with_global()  # ê¸€ë¡œë²Œ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”

        # ê²½í—˜ ë²„í¼
        self.buffer_s = []
        self.buffer_a = []
        self.buffer_r = []
        self.buffer_logp = []

    def sync_with_global(self):
        """ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ë¡œë¶€í„° ìµœì‹  íŒŒë¼ë¯¸í„° ë³µì‚¬"""
        self.global_agent.sync_to_local(self.actor, self.critic)

    def compute_td_target(self, reward, next_state, done):
        """1-step TD íƒ€ê¹ƒ ê³„ì‚°"""
        with torch.no_grad():
            if done:
                return torch.tensor([[reward]], dtype=torch.float64, device=device)
            ns = torch.from_numpy(next_state).to(device).unsqueeze(0).to(torch.float64)
            v_next = self.critic(ns)
            return reward + gamma * v_next

    def push_transition(self, s, a, r, logp):
        """ë²„í¼ì— transition ì €ì¥"""
        self.buffer_s.append(s)
        self.buffer_a.append(a)
        self.buffer_r.append(r)
        self.buffer_logp.append(logp)

    def clear_buffers(self):
        """ë²„í¼ ì´ˆê¸°í™”"""
        self.buffer_s.clear()
        self.buffer_a.clear()
        self.buffer_r.clear()
        self.buffer_logp.clear()

    def run(self):
        """ì›Œì»¤ì˜ ë©”ì¸ ë£¨í”„"""
        global GLOBAL_EP

        while True:
            # ---- ì¢…ë£Œ ì¡°ê±´ ì²´í¬ ----
            with GLOBAL_EP_LOCK:
                if GLOBAL_EP >= max_episodes:
                    break
                ep_idx = GLOBAL_EP + 1  # í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸

            # í™˜ê²½ ì´ˆê¸°í™”
            s = reset_env(self.env)
            ep_ret = 0.0
            done = False
            self.clear_buffers()

            # ---- í•œ ì—í”¼ì†Œë“œ ì‹¤í–‰ ----
            while not done:
                # í˜„ì¬ ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜
                st = torch.from_numpy(np.asarray(s)).to(device).unsqueeze(0).to(torch.float64)
                # ì•¡ì…˜ ìƒ˜í”Œë§
                action_np, logp_t = self.actor.sample_action(st)
                action_env = np.asarray(action_np, dtype=np.float64)

                # í™˜ê²½ ìŠ¤í…
                ns, r, done, _ = step_env(self.env, action_env)
                ep_ret += r

                # ë³´ìƒ ìŠ¤ì¼€ì¼ë§ (TF ì½”ë“œì˜ (r+8)/8)
                r_scaled = (r + 8.0) / 8.0

                # ë²„í¼ ì €ì¥
                self.push_transition(
                    s=np.asarray(s, dtype=np.float64),
                    a=action_np,
                    r=r_scaled,
                    logp=logp_t.cpu().numpy()
                )

                s = ns

                # ì¼ì • ìŠ¤í…ë§ˆë‹¤ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
                if len(self.buffer_s) >= update_interval or done:
                    self.update_global(ns, done)
                    self.sync_with_global()
                    self.clear_buffers()

            # ---- ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ----
            with GLOBAL_EP_LOCK:
                GLOBAL_EP += 1
                ep_no = GLOBAL_EP

            with PRINT_LOCK:
                print(f"{self.name} | EP{ep_no} EpisodeReward={ep_ret:.2f}")

    def update_global(self, next_state, done):
        """ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ì—ì„œ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê³  ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ì— ì ìš©"""
        # numpy â†’ torch ë³€í™˜
        states = torch.from_numpy(np.vstack(self.buffer_s)).to(device).to(torch.float64)
        actions = torch.from_numpy(np.vstack(self.buffer_a)).to(device).to(torch.float64)
        logps_old = torch.from_numpy(np.vstack(self.buffer_logp)).to(device).to(torch.float64)
        rewards = torch.from_numpy(np.vstack(self.buffer_r)).to(device).to(torch.float64)

        # ---- n-step TD íƒ€ê¹ƒ ê³„ì‚° ----
        with torch.no_grad():
            if done:
                v_next = torch.zeros((1, 1), dtype=torch.float64, device=device)
            else:
                ns_t = torch.from_numpy(np.asarray(next_state)).to(device).unsqueeze(0).to(torch.float64)
                v_next = self.critic(ns_t)

        # ë¦¬í„´(ëˆ„ì  ë³´ìƒ) ê³„ì‚° (ë’¤ì—ì„œë¶€í„° discount ëˆ„ì )
        returns = []
        R = v_next.squeeze(0)
        for r in reversed(rewards):
            R = r + gamma * R
            returns.append(R)
        returns = returns[::-1]
        returns = torch.stack(returns).unsqueeze(-1)  # [T, 1] í˜•íƒœ

        # í˜„ì¬ ê°€ì¹˜ V(s)
        values = self.critic(states)
        advantages = returns - values  # A(s,a) = R - V(s)

        # ì •ì±… ë¡œê·¸í™•ë¥  ë° ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        logp, entropy = self.actor.log_prob_and_entropy(states, actions)

        # ---- ì†ì‹¤ ì •ì˜ ----
        # (1) Actor: -E[logÏ€(a|s) * A] - Î² * Entropy
        actor_loss = -(logp * advantages.detach()).mean() - entropy_beta * entropy.mean()
        # (2) Critic: MSE(R, V)
        critic_loss = 0.5 * (returns.detach() - values).pow(2).mean()

        # ---- ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ ê°±ì‹  ----
        self.global_agent.apply_grads(actor_loss, critic_loss, self.actor.parameters(), self.critic.parameters())


# ===============================================================
# ğŸ§  A3C Agent í´ë˜ìŠ¤ (ìŠ¤ë ˆë“œ ì‹¤í–‰ ì œì–´)
# ===============================================================
class A3CAgent:
    def __init__(self, env_name: str):
        self.env_name = env_name
        self.global_agent = GlobalA3C(env_name)
        self.num_workers = cpu_count()  # ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜

    def train(self):
        """ëª¨ë“  ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘"""
        print(f"Training on {self.num_workers} cores (threads)")
        workers = [Worker(i, self.env_name, self.global_agent) for i in range(self.num_workers)]
        for w in workers:
            w.start()
        for w in workers:
            w.join()



@torch.no_grad()
def evaluate_and_record(env_name: str,
                        actor_path: str,
                        critic_path: str,
                        out_dir: str = "videos",
                        episodes: int = 3,
                        max_steps: int = 2000):
    """
    ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í‰ê°€í•˜ê³ , ê° ì—í”¼ì†Œë“œë¥¼ ì˜ìƒìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # ë¹„ë””ì˜¤ í´ë” ì¤€ë¹„
    pathlib.Path(out_dir).mkdir(parents=True, exist_ok=True)

    # ë¹„ë””ì˜¤ ë…¹í™” ê°€ëŠ¥í•œ env ìƒì„± (rgb_array í•„ìˆ˜)
    base_env = gym.make(env_name, render_mode="rgb_array")
    env = RecordVideo(base_env, video_folder=out_dir, episode_trigger=lambda e: True)

    # ì•¡ì…˜ ë²”ìœ„/ìƒíƒœ í¬ê¸° í™•ì¸ì„ ìœ„í•´ ì„ì‹œ í™˜ê²½ì—ì„œ ì •ë³´ ì–»ê¸°
    tmp = gym.make(env_name)
    state_size = tmp.observation_space.shape[0]
    action_size = tmp.action_space.shape[0]
    action_bound = float(tmp.action_space.high[0])
    tmp.close()

    # ë™ì¼ ì•„í‚¤í…ì²˜ ëª¨ë¸ ìƒì„± í›„ ë¡œë“œ
    actor = Actor(state_size, action_size, action_bound).to(device)
    critic = Critic(state_size).to(device)
    actor.load_state_dict(torch.load(actor_path, map_location=device))
    critic.load_state_dict(torch.load(critic_path, map_location=device))
    actor.eval(); critic.eval()

    def select_action_mu(state_np: np.ndarray) -> np.ndarray:
        """í‰ê°€ìš©: í‰ê·  í–‰ë™(Î¼) ì‚¬ìš© (deterministic)"""
        s = torch.from_numpy(state_np).to(device).unsqueeze(0).to(torch.float64)
        mu, std = actor(s)
        a = mu.clamp(-action_bound, action_bound)  # ì•ˆì „ í´ë¨í”„
        return a.squeeze(0).cpu().numpy()

    # ì—í”¼ì†Œë“œ ë£¨í”„
    for ep in range(episodes):
        # resetì€ gym/gymnasium í˜¸í™˜ ì²˜ë¦¬
        out = env.reset()
        state = out[0] if isinstance(out, tuple) else out
        done = False
        total_r = 0.0

        for t in range(max_steps):
            action = select_action_mu(state)
            # stepë„ ë²„ì „ë³„ ë°˜í™˜ í˜•íƒœë¥¼ ê°ì•ˆí•´ ì•ˆì „ ì²˜ë¦¬
            out = env.step(action)
            if len(out) == 5:
                next_state, reward, terminated, truncated, info = out
                done = bool(terminated or truncated)
            else:
                next_state, reward, done, info = out
            total_r += reward
            state = next_state
            if done:
                break

        print(f"[EVAL] Episode {ep+1}/{episodes} return={total_r:.2f}")

    env.close()
# ===============================================================
# ğŸš€ ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# ===============================================================
if __name__ == "__main__":
    env_name = "Pendulum-v1"  # ì—°ì† ì œì–´ í™˜ê²½
    agent = A3CAgent(env_name)

    # --- (A) í•™ìŠµ ---
    DO_TRAIN = False
    if DO_TRAIN:
        agent.train()
        # ëª¨ë¸ ì €ì¥ (ê²½ë¡œëŠ” ììœ ë¡­ê²Œ ë³€ê²½)
        actor_path = "a3c_actor.pth"
        critic_path = "a3c_critic.pth"
        agent.global_agent.save(actor_path, critic_path)
        print(f"[SAVE] Saved to {actor_path}, {critic_path}")

    # --- (B) ë¡œë“œ í›„ ë¹„ë””ì˜¤ í‰ê°€/ì €ì¥ ---
    DO_RECORD = True
    if DO_RECORD:
        # í•™ìŠµ ì§í›„ê°€ ì•„ë‹ˆë¼ë©´, ìƒˆ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¶ˆëŸ¬ì™€ë„ ë©ë‹ˆë‹¤:
        # loader = GlobalA3C(env_name)
        # loader.load("a3c_actor.pth", "a3c_critic.pth")

        evaluate_and_record(
            env_name=env_name,
            actor_path="Video/continuous/a3c_actor.pth",
            critic_path="Video/continuous/a3c_critic.pth",
            out_dir="videos",  # ë¹„ë””ì˜¤ ì €ì¥ í´ë”
            episodes=3,  # ì €ì¥í•  ì—í”¼ì†Œë“œ ìˆ˜
            max_steps=2000  # ì—í”¼ì†Œë“œ ìµœëŒ€ ìŠ¤í…
        )
        print("[VIDEO] Saved evaluation videos under ./videos")