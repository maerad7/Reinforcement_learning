# ================================================================
# ğŸ§  PyTorch A3C (CartPole-v1) - Best Score Auto Save (Train + Eval)
#   - dtype=float32 í†µì¼
#   - ë©€í‹°ìŠ¤ë ˆë“œ ë½ ë™ê¸°í™”
#   - í›ˆë ¨/í‰ê°€ ìµœê³  ì ìˆ˜ ì‹œ ìë™ ì €ì¥
#   - ìˆ˜ë™ ì €ì¥/ë¡œë“œ + ë¹„ë””ì˜¤ ë…¹í™”(RecordVideo)
# ================================================================

import os, time
from typing import Tuple
from threading import Thread, Lock
from multiprocessing import cpu_count

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
from gymnasium.wrappers import RecordVideo

# ================================================================
# Global ì„¤ì •
# ================================================================
torch.set_default_dtype(torch.float32)                # ëª¨ë“  tensor ê¸°ë³¸ dtypeì„ float32ë¡œ ê³ ì •í•´ í˜¼ìš© ë¬¸ì œ ë°©ì§€
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # CUDA ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU, ì•„ë‹ˆë©´ CPU

actor_lr = 5e-4                                       # Actor(ì •ì±…) ë„¤íŠ¸ì›Œí¬ í•™ìŠµë¥ 
critic_lr = 1e-3                                      # Critic(ê°€ì¹˜) ë„¤íŠ¸ì›Œí¬ í•™ìŠµë¥ 
gamma = 0.99                                          # ë³´ìƒ í• ì¸ìœ¨ Î³
hidden_size = 128                                     # MLP ì€ë‹‰ì°¨ì›
update_interval = 50                                  # ë¡œì»¬ ë²„í¼ ê¸¸ì´: ì´ ê¸¸ì´ì— ë„ë‹¬í•˜ê±°ë‚˜ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸€ë¡œë²Œ ì—…ë°ì´íŠ¸
max_episodes = 500                                    # ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
entropy_beta = 1e-2                                   # ì •ì±… ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜(íƒí—˜ ìœ ë„)
EVAL_EVERY = 20                                       # N ì—í”¼ì†Œë“œë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰

GLOBAL_EP = 0                                         # í˜„ì¬ê¹Œì§€ ì™„ë£Œëœ ì—í”¼ì†Œë“œ ì¹´ìš´í„°(ì „ì—­)
GLOBAL_EP_LOCK = Lock()                               # GLOBAL_EP ì¦ê°€ë¥¼ ì›ìì ìœ¼ë¡œ ë³´í˜¸
PRINT_LOCK = Lock()                                   # ì½˜ì†” ì¶œë ¥ì´ ìŠ¤ë ˆë“œ ê°„ ì„ì´ì§€ ì•Šë„ë¡ ë³´í˜¸
UPDATE_LOCK = Lock()                                  # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì„ê³„ì˜ì—­ ë³´í˜¸

# ================================================================
# Gym Helper í•¨ìˆ˜
# ================================================================
def reset_env(env):
    out = env.reset()                                 # Gymnasiumì€ (obs, info) íŠœí”Œ ë°˜í™˜
    return out[0] if isinstance(out, tuple) else out  # ê´€ì¸¡ì¹˜(obs)ë§Œ ì‚¬ìš©

def step_env(env, action) -> Tuple[np.ndarray, float, bool, dict]:
    out = env.step(action)                            # Gymnasiumì€ (obs, reward, terminated, truncated, info)
    if len(out) == 5:                                 # ìµœì‹  Gymnasium í¬ë§·
        ns, r, term, trunc, info = out
        return ns, float(r), bool(term or trunc), info  # term ë˜ëŠ” truncê°€ Trueë©´ done
    else:                                             # (êµ¬ë²„ì „ í˜¸í™˜) (obs, reward, done, info)
        ns, r, done, info = out
        return ns, float(r), bool(done), info

# ================================================================
# Actor ì •ì˜
# ================================================================
class Actor(nn.Module):
    def __init__(self, state_size, action_size):
        super().__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)      # ì…ë ¥: ìƒíƒœ ë²¡í„° â†’ ì€ë‹‰
        self.fc2 = nn.Linear(hidden_size, hidden_size)     # ì€ë‹‰ â†’ ì€ë‹‰
        self.policy_head = nn.Linear(hidden_size, action_size)  # ì€ë‹‰ â†’ ê° í–‰ë™ì˜ ë¡œì§“
        self.softmax = nn.Softmax(dim=-1)                  # ë¡œì§“ â†’ í™•ë¥ 
        self.opt = optim.Adam(self.parameters(), lr=actor_lr)   # ì •ì±… ìµœì í™”ê¸°
        self.entropy_beta = entropy_beta                    # ì—”íŠ¸ë¡œí”¼ ê°€ì¤‘ì¹˜ ì €ì¥

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = torch.relu(self.fc1(x))                         # ReLU ë¹„ì„ í˜•ì„±
        x = torch.relu(self.fc2(x))
        logits = self.policy_head(x)                        # í–‰ë™ë³„ ë¡œì§“
        return self.softmax(logits)                         # ì •ê·œí™”ëœ í–‰ë™ í™•ë¥  Ï€(a|s)

    def compute_loss(self, probs, actions, advantages):
        dist = torch.distributions.Categorical(probs)       # ì´ì‚° í–‰ë™ ê³µê°„ìš© ë²”ì£¼ë¶„í¬
        log_probs = dist.log_prob(actions.squeeze(-1))      # ì„ íƒí•œ í–‰ë™ì˜ log Ï€(a|s)
        entropy = dist.entropy().mean()                     # í‰ê·  ì—”íŠ¸ë¡œí”¼(íƒí—˜ì„± ì²™ë„)
        policy_loss = -(log_probs * advantages.squeeze(-1)).mean()  # A3C ì •ì±…ì†ì‹¤ = -E[logÏ€ * A]
        return policy_loss - self.entropy_beta * entropy    # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ë¡œ íƒí—˜ì„± ìœ ì§€

    def train_step(self, states, actions, advantages):
        probs = self.forward(states)                        # ë¯¸ë‹ˆë°°ì¹˜ ìƒíƒœ â†’ í–‰ë™í™•ë¥ 
        loss = self.compute_loss(probs, actions, advantages)# ì •ì±… ì†ì‹¤ ê³„ì‚°
        self.opt.zero_grad()                                # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        loss.backward()                                     # ì—­ì „íŒŒ
        self.opt.step()                                     # íŒŒë¼ë¯¸í„° ê°±ì‹ 
        return float(loss.item())

# ================================================================
# Critic ì •ì˜
# ================================================================
class Critic(nn.Module):
    def __init__(self, state_size):
        super().__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)       # ìƒíƒœ â†’ ì€ë‹‰
        self.fc2 = nn.Linear(hidden_size, hidden_size)      # ì€ë‹‰ â†’ ì€ë‹‰
        self.v_head = nn.Linear(hidden_size, 1)             # ì€ë‹‰ â†’ ìŠ¤ì¹¼ë¼ V(s)
        self.opt = optim.Adam(self.parameters(), lr=critic_lr)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.v_head(x)                               # ìƒíƒœê°€ì¹˜ V(s)

    def train_step(self, states, td_targets):
        values = self.forward(states)                       # V(s) ì˜ˆì¸¡
        loss = torch.mean((td_targets - values) ** 2)       # TD-íƒ€ê¹ƒê³¼ì˜ MSE
        self.opt.zero_grad()
        loss.backward()
        self.opt.step()
        return float(loss.item())

# ================================================================
# A3CAgent (ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ + Best ì €ì¥ ë¡œì§)
# ================================================================
class A3CAgent:
    def __init__(self, env_name: str, gamma: float):
        self.env_name = env_name
        self.gamma = gamma

        # í™˜ê²½ ì •ë³´ í™•ì¸(ìƒíƒœ/í–‰ë™ ì°¨ì› íŒŒì•…ìš© ë”ë¯¸ env)
        tmp = gym.make(env_name)
        self.state_size = tmp.observation_space.shape[0]    # CartPole: 4ì°¨ì›
        self.action_size = tmp.action_space.n               # CartPole: 2ê°œ(ì¢Œ/ìš°)
        tmp.close()

        # ì „ì—­ ê³µìœ  ë„¤íŠ¸ì›Œí¬(Actor, Critic)
        self.global_actor = Actor(self.state_size, self.action_size).to(device).float()
        self.global_critic = Critic(self.state_size).to(device).float()

        # ì›Œì»¤ ìˆ˜: CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜(í•„ìš”ì‹œ ìƒí•œ ì œí•œ ê°€ëŠ¥)
        self.num_workers = cpu_count()

        # ìµœê³ ì  ì €ì¥ ê´€ë¦¬ ë³€ìˆ˜
        self.best_score = float("-inf")                     # í˜„ì¬ê¹Œì§€ ìµœê³  í‰ê· /ë¦¬í„´
        self.best_lock = Lock()                             # ë™ì‹œ ì ‘ê·¼ ë³´í˜¸
        self.best_actor_path = "a3c_cartpole_actor_best.pth"
        self.best_critic_path = "a3c_cartpole_critic_best.pth"

    # -----------------------------
    # í†µí•© ë² ìŠ¤íŠ¸ ì €ì¥ í•¨ìˆ˜
    # -----------------------------
    def save_best(self, score: float, tag: str = "train"):
        """í›ˆë ¨/í‰ê°€ì—ì„œ ì–»ì€ scoreê°€ ìµœê³ ì¹˜ë©´ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ì €ì¥"""
        with self.best_lock:                                # ë‹¤ì¤‘ ìŠ¤ë ˆë“œ ë³´í˜¸
            if score > self.best_score:                     # ìµœê³  ê¸°ë¡ ê°±ì‹  ì‹œ
                self.best_score = score
                torch.save(self.global_actor.state_dict(), self.best_actor_path)
                torch.save(self.global_critic.state_dict(), self.best_critic_path)
                with PRINT_LOCK:
                    print(f"[BEST-{tag.upper()}] New best {score:.2f} saved "
                          f"({self.best_actor_path}, {self.best_critic_path})")

    # -----------------------------
    # í•™ìŠµ ì‹¤í–‰ (ì›Œì»¤ ë³‘ë ¬)
    # -----------------------------
    def train(self):
        print(f"Training on {self.num_workers} cores")      # ì‚¬ìš© ì½”ì–´ ìˆ˜ ì•ˆë‚´
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ìƒì„±
        workers = [Worker(i, self.env_name, self.gamma, self) for i in range(self.num_workers)]
        for w in workers: w.start()                         # ê° ì›Œì»¤ ì‹œì‘(daemon=True)
        for w in workers: w.join()                          # ëª¨ë“  ì›Œì»¤ ì¢…ë£Œê¹Œì§€ ëŒ€ê¸°

    def save(self, actor_path, critic_path):
        """ë§ˆì§€ë§‰ ëª¨ë¸ ìˆ˜ë™ ì €ì¥"""
        torch.save(self.global_actor.state_dict(), actor_path)
        torch.save(self.global_critic.state_dict(), critic_path)

    def load(self, actor_path, critic_path, map_location=None):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ + í‰ê°€ëª¨ë“œ ì „í™˜"""
        map_location = map_location or device
        self.global_actor.load_state_dict(torch.load(actor_path, map_location=map_location))
        self.global_critic.load_state_dict(torch.load(critic_path, map_location=map_location))
        self.global_actor.eval()
        self.global_critic.eval()

    # -----------------------------
    # ì •ì±… í‰ê°€ (í‰ê·  ë¦¬í„´ ê³„ì‚°)
    # -----------------------------
    @torch.no_grad()
    def evaluate_policy(self, episodes=5, max_steps=500, seed=42):
        env = gym.make(self.env_name, max_episode_steps=max_steps)  # í‰ê°€ìš© env
        actor = Actor(self.state_size, self.action_size).to(device).float()
        actor.load_state_dict(self.global_actor.state_dict())       # ê¸€ë¡œë²Œ ì •ì±… ìŠ¤ëƒ…ìƒ·
        actor.eval()

        def greedy_action(s_np):
            """ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í–‰ë™ ì„ íƒ(íƒìš•)"""
            s = torch.tensor(s_np, dtype=torch.float32, device=device).unsqueeze(0)
            return int(torch.argmax(actor(s), dim=-1).item())

        returns = []                                               # ì—í”¼ì†Œë“œë³„ ì´ë³´ìƒ ì €ì¥
        for ep in range(episodes):
            s, _ = env.reset(seed=seed + ep)                       # ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± í™•ë³´
            done, ep_ret = False, 0.0
            for _ in range(max_steps):
                a = greedy_action(s)                               # íƒìš• ì‹¤í–‰
                s, r, term, trunc, _ = env.step(a)
                done = term or trunc
                ep_ret += float(r)
                if done: break
            returns.append(ep_ret)
        env.close()

        avg = float(np.mean(returns))                              # í‰ê·  ë¦¬í„´
        with PRINT_LOCK:
            print(f"[EVAL] avg_return={avg:.2f}")
        self.save_best(avg, tag="eval")                            # í‰ê°€ ê¸°ì¤€ ë² ìŠ¤íŠ¸ ì €ì¥
        return avg

# ================================================================
# Worker í´ë˜ìŠ¤ (ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ + ê¸€ë¡œë²Œ ê°±ì‹ )
# ================================================================
class Worker(Thread):
    def __init__(self, wid, env_name, gamma, agent_ref):
        super().__init__(daemon=True)                              # ë©”ì¸ ì¢…ë£Œ ì‹œ ìë™ ì¢…ë£Œ
        self.wid = wid
        self.name = f"w{wid}"                                      # ë¡œê·¸ ì‹ë³„ìš© ì´ë¦„
        self.env = gym.make(env_name)                              # ê° ì›Œì»¤ ë…ë¦½ í™˜ê²½
        self.gamma = gamma
        self.agent = agent_ref

        # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ í•¸ë“¤
        self.global_actor = agent_ref.global_actor
        self.global_critic = agent_ref.global_critic

        # ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ ìƒì„±(ì´ˆê¸°ì—” ê¸€ë¡œë²Œ íŒŒë¼ë¯¸í„°ë¡œ ë™ê¸°í™”)
        self.state_size = self.env.observation_space.shape[0]
        self.action_size = self.env.action_space.n
        self.actor = Actor(self.state_size, self.action_size).to(device).float()
        self.critic = Critic(self.state_size).to(device).float()
        self.sync_with_global()                                     # ì´ˆê¸° ë™ê¸°í™”

    def sync_with_global(self):
        """ê¸€ë¡œë²Œ â†’ ë¡œì»¬ íŒŒë¼ë¯¸í„° ë³µì‚¬(ë™ê¸°í™”)"""
        self.actor.load_state_dict(self.global_actor.state_dict())
        self.critic.load_state_dict(self.global_critic.state_dict())

    def get_action(self, state_np):
        """í˜„ì¬ ì •ì±…ì— ë”°ë¼ í™•ë¥ ì ìœ¼ë¡œ í–‰ë™ ìƒ˜í”Œ"""
        s = torch.tensor(state_np, dtype=torch.float32, device=device).unsqueeze(0)
        probs = self.actor(s)                                       # Ï€(a|s)
        dist = torch.distributions.Categorical(probs)
        return int(dist.sample().item())                            # ìƒ˜í”Œë§ìœ¼ë¡œ íƒí—˜ ë°˜ì˜

    def n_step_td_target(self, rewards_np, next_v, done):
        """
        n-step TD íƒ€ê¹ƒ ê³„ì‚°.
        - rewards_np: shape (n, 1), ê²½ë¡œì—ì„œ ëª¨ì€ ë³´ìƒë“¤
        - next_v: ë§ˆì§€ë§‰ ë‹¤ìŒ ìƒíƒœì˜ V(s_{t+n}), ì¢…ë£Œë©´ 0
        - done: ì¢…ë£Œ ì—¬ë¶€
        """
        td_targets = np.zeros_like(rewards_np, dtype=np.float32)
        R_to_go = 0.0 if done else float(next_v)                    # ì¢…ë£Œë©´ bootstrap ì—†ìŒ
        for k in reversed(range(len(rewards_np))):                  # ë’¤ì—ì„œë¶€í„° ëˆ„ì  í• ì¸í•©
            R_to_go = float(rewards_np[k, 0]) + self.gamma * R_to_go
            td_targets[k, 0] = R_to_go
        return td_targets

    def run(self):
        """ì›Œì»¤ì˜ ë©”ì¸ ë£¨í”„: í™˜ê²½ ì‹¤í–‰ â†’ ë¡œì»¬ ë²„í¼ ì ì¬ â†’ ì£¼ê¸°ì  ê¸€ë¡œë²Œ ì—…ë°ì´íŠ¸"""
        global GLOBAL_EP
        while True:
            # ì—í”¼ì†Œë“œ ì¸ë±ìŠ¤ ì½ê¸°/ì¢…ë£Œ ê²€ì‚¬(ì„ê³„ì˜ì—­)
            with GLOBAL_EP_LOCK:
                if GLOBAL_EP >= max_episodes:
                    break
                ep_idx = GLOBAL_EP + 1                              # (í•„ìš”ì‹œ ë¡œê¹…ìš©)

            state = reset_env(self.env)
            done, ep_return = False, 0.0
            states, actions, rewards = [], [], []                   # ë¡¤ì•„ì›ƒ ë²„í¼

            while not done:
                action = self.get_action(state)                     # ì •ì±…ì— ë”°ë¥¸ í–‰ë™ ì„ íƒ
                next_state, reward, done, _ = step_env(self.env, action)
                ep_return += reward
                states.append(state)                                # s_t
                actions.append([action])                            # a_t (ì—´ë²¡í„° í˜•íƒœ)
                rewards.append([reward])                            # r_{t+1} (ì—´ë²¡í„°)
                state = next_state

                # ë°°ì¹˜ê°€ ì°¨ê±°ë‚˜ ì¢…ë£Œë˜ë©´ ê¸€ë¡œë²Œ ì—…ë°ì´íŠ¸
                if len(states) >= update_interval or done:
                    states_t  = torch.tensor(np.vstack(states), dtype=torch.float32, device=device)
                    actions_t = torch.tensor(np.vstack(actions), dtype=torch.int64, device=device)
                    rewards_np = np.vstack(rewards).astype(np.float32)

                    with torch.no_grad():
                        curr_Vs = self.critic(states_t).detach().cpu().numpy()  # í˜„ì¬ ë¡œì»¬ Criticì˜ V(s_t)
                        next_v = 0.0
                        if not done:
                            ns_t = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)
                            next_v = float(self.critic(ns_t).item())             # bootstrap V(s_{t+n})

                    td_targets_np = self.n_step_td_target(rewards_np, next_v, done)  # n-step íƒ€ê¹ƒ
                    td_targets_t  = torch.tensor(td_targets_np, dtype=torch.float32, device=device)
                    advantages_t  = td_targets_t - torch.tensor(curr_Vs, dtype=torch.float32, device=device)  # A = G_t - V(s)

                    # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸(ì„ê³„ì˜ì—­ ë³´í˜¸)
                    with UPDATE_LOCK:
                        self.global_actor.train_step(states_t, actions_t, advantages_t)
                        self.global_critic.train_step(states_t, td_targets_t)

                    self.sync_with_global()                            # ì—…ë°ì´íŠ¸ í›„ ë¡œì»¬ ì¬ë™ê¸°í™”
                    states, actions, rewards = [], [], []              # ë²„í¼ ì´ˆê¸°í™”

            # --- ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œì : ìµœê³  ë¦¬í„´ ì €ì¥ ì‹œë„ ---
            self.agent.save_best(ep_return, tag="train")               # í›ˆë ¨ ë¦¬í„´ ê¸°ì¤€

            # ì „ì—­ ì—í”¼ì†Œë“œ ì¹´ìš´í„° ì¦ê°€(ì„ê³„ì˜ì—­)
            with GLOBAL_EP_LOCK:
                GLOBAL_EP += 1
                ep_no = GLOBAL_EP

            # --- ì£¼ê¸°ì  í‰ê°€ (ì›Œì»¤ 0ë§Œ ìˆ˜í–‰í•´ì„œ ì¤‘ë³µ ë°©ì§€) ---
            if (ep_no % EVAL_EVERY == 0) and (self.wid == 0):
                with UPDATE_LOCK:                                      # í‰ê°€ ì§ì „/ì¤‘ íŒŒë¼ë¯¸í„° ê³ ì •
                    self.agent.evaluate_policy(episodes=5, max_steps=500, seed=1234)

            # ì—í”¼ì†Œë“œ ë¡œê·¸ ì¶œë ¥(ìŠ¤ë ˆë“œ ì•ˆì „)
            with PRINT_LOCK:
                print(f"{self.name} | EP{ep_no} Return={ep_return:.2f}")

# ================================================================
# í‰ê°€ + ë¹„ë””ì˜¤ ì €ì¥
# ================================================================
@torch.no_grad()
def evaluate_and_record_discrete(env_name, actor_path, out_dir="videos",
                                 episodes=5, max_steps=500, seed=42, greedy=True):
    """
    ì €ì¥ëœ Actor ê°€ì¤‘ì¹˜ë¡œ ì—í”¼ì†Œë“œ ì‹¤í–‰í•˜ë©° ë¹„ë””ì˜¤ ì €ì¥.
    - greedy=True: argmax ì •ì±…ìœ¼ë¡œ ì‹¤í–‰(ì„±ëŠ¥ í™•ì¸ìš©)
    - greedy=False: í™•ë¥  ìƒ˜í”Œë§ìœ¼ë¡œ ì‹¤í–‰(ë‹¤ì–‘ì„± í™•ì¸ìš©)
    """
    stamp = time.strftime("%Y%m%d_%H%M%S")                           # ê²°ê³¼ í´ë” íƒ€ì„ìŠ¤íƒ¬í”„
    video_dir = os.path.join(out_dir, stamp)
    os.makedirs(video_dir, exist_ok=True)

    # RecordVideo ë˜í¼: ë§¤ ì—í”¼ì†Œë“œ ë¹„ë””ì˜¤ ì €ì¥(episode_trigger=lambda e: True)
    env = RecordVideo(
        gym.make(env_name, render_mode="rgb_array", max_episode_steps=max_steps),
        video_folder=video_dir, episode_trigger=lambda e: True
    )

    # Actor êµ¬ì¡° ìƒì„±ì„ ìœ„í•´ ìƒíƒœ/í–‰ë™ ì°¨ì› í™•ì¸
    tmp = gym.make(env_name)
    state_size = tmp.observation_space.shape[0]
    action_size = tmp.action_space.n
    tmp.close()

    # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    actor = Actor(state_size, action_size).to(device).float()
    actor.load_state_dict(torch.load(actor_path, map_location=device))
    actor.eval()

    def select_action(state_np):
        """greedy(Argmax) ë˜ëŠ” stochastic(Categorical.sample) ì„ íƒ"""
        s = torch.tensor(state_np, dtype=torch.float32, device=device).unsqueeze(0)
        probs = actor(s)
        return int(torch.argmax(probs, dim=-1)) if greedy else int(torch.distributions.Categorical(probs).sample().item())

    # ì—í”¼ì†Œë“œ ì‹¤í–‰ + ë¹„ë””ì˜¤ ê¸°ë¡
    for ep in range(episodes):
        s, _ = env.reset(seed=seed + ep)
        done, ep_ret = False, 0.0
        for t in range(max_steps):
            a = select_action(s)
            s, r, term, trunc, _ = env.step(a)
            done = term or trunc
            ep_ret += float(r)
            if done: break
        print(f"[EVAL] Ep {ep+1}/{episodes} return={ep_ret:.2f}")

    env.close()
    print(f"[VIDEO] Saved under: {video_dir}")                        # ë¹„ë””ì˜¤ ì €ì¥ ê²½ë¡œ ì¶œë ¥

# ================================================================
# Main ì‹¤í–‰
# ================================================================
if __name__ == "__main__":
    env_name = "CartPole-v1"
    agent = A3CAgent(env_name, gamma)                                 # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬/ê´€ë¦¬ì ì´ˆê¸°í™”

    DO_TRAIN = True
    if DO_TRAIN:
        agent.train()                                                 # ë³‘ë ¬ A3C í•™ìŠµ ì‹œì‘
        agent.save("a3c_actor_last.pth", "a3c_critic_last.pth")       # ë§ˆì§€ë§‰ ìŠ¤ëƒ…ìƒ· ì €ì¥
        print(f"[SAVE-LAST] Last models saved.")
        print(f"[BEST] best_score={agent.best_score:.2f}")            # ì„¸ì…˜ ìµœê³  ë¦¬í„´ ê¸°ë¡ í‘œì‹œ

    DO_RECORD = True
    if DO_RECORD:
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸(í›ˆë ¨/í‰ê°€ ê¸°ì¤€)ì„ ì‚¬ìš©í•´ ë¹„ë””ì˜¤ ë…¹í™”
        evaluate_and_record_discrete(env_name, actor_path=agent.best_actor_path,
                                     out_dir="videos", episodes=3, max_steps=500)
